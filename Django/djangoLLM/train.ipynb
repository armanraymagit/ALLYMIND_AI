{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DPO Training Notebook\n",
                "\n",
                "This notebook allows you to train an LLM using Direct Preference Optimization (DPO). parameters are configured in the `Config` class below for easy interactive modification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "from datasets import Dataset\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "from trl import DPOTrainer, DPOConfig\n",
                "from peft import LoraConfig"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Config:\n",
                "    \"\"\"\n",
                "    Configuration class to replace argparse for notebook usage.\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        # Model & Data Paths\n",
                "        self.base_model_path = \"mistralai/Mistral-7B-v0.1\" # Replace with your model path\n",
                "        self.data_path = \"data.json\" # Replace with your data path\n",
                "        self.output_dir = \"./results\"\n",
                "        \n",
                "        # Training Hyperparameters\n",
                "        self.num_train_epochs = 3\n",
                "        self.learning_rate = 5e-5\n",
                "        self.batch_size = 2\n",
                "        \n",
                "        # LoRA Configuration\n",
                "        self.lora_r = 16\n",
                "        self.lora_alpha = 32\n",
                "        self.lora_dropout = 0.05\n",
                "        \n",
                "        # Text Generation / Processing\n",
                "        self.max_length = 1024\n",
                "        self.max_prompt_length = 512\n",
                "        \n",
                "        # DPO Specific\n",
                "        self.beta = 0.1\n",
                "        \n",
                "        # Optimization\n",
                "        self.warmup_steps = 100\n",
                "        self.warmup_ratio = 0.1\n",
                "        self.max_grad_norm = 0.3\n",
                "\n",
                "args = Config()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def training_data_processor(args):\n",
                "    \"\"\"\n",
                "    Placeholder for processing training data.\n",
                "    In a real scenario, this would load and format your dataset.\n",
                "    For DPO, you need 'prompt', 'chosen', and 'rejected' columns.\n",
                "    \"\"\"\n",
                "    print(f\"Loading data from {args.data_path}\")\n",
                "    # Example dummy data for demonstration\n",
                "    prompts = [\n",
                "        \"What is the capital of France?\",\n",
                "        \"Tell me about large language models.\",\n",
                "    ]\n",
                "    chosen = [\n",
                "        \"The capital of France is Paris.\",\n",
                "        \"Large language models (LLMs) are deep learning models that can understand and generate human-like text.\",\n",
                "    ]\n",
                "    rejected = [\n",
                "        \"France's capital is Berlin.\",\n",
                "        \"LLMs are small, simple neural networks.\",\n",
                "    ]\n",
                "\n",
                "    return {\"prompt\": prompts, \"chosen\": chosen, \"rejected\": rejected}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(args.base_model_path, padding_side=\"left\")\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token # Or another suitable token"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "# Use torch.float32 for CPU training, bfloat16 for supported GPUs\n",
                "if torch.cuda.is_available():\n",
                "    torch_dtype = torch.bfloat16 # or torch.float16 depending on GPU support\n",
                "    print(\"CUDA available, using bfloat16 for model.\")\n",
                "else:\n",
                "    torch_dtype = torch.float32\n",
                "    print(\"CUDA not available, using float32 for model.\")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    args.base_model_path,\n",
                "    trust_remote_code=True,\n",
                "    ignore_mismatched_sizes=True,\n",
                "    torch_dtype=torch_dtype,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare training data\n",
                "data_dict = training_data_processor(args)\n",
                "dataset = Dataset.from_dict(data_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LoRA configuration\n",
                "lora_config = None\n",
                "if args.lora_r > 0:\n",
                "    lora_config = LoraConfig(\n",
                "        r=args.lora_r,\n",
                "        lora_alpha=args.lora_alpha,\n",
                "        lora_dropout=args.lora_dropout,\n",
                "        bias=\"none\",\n",
                "        target_modules=\"all-linear\",\n",
                "        task_type=\"CAUSAL_LM\",\n",
                "    )\n",
                "else:\n",
                "    print(\"LoRA disabled (lora_r is 0).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DPO training arguments\n",
                "training_args = DPOConfig(\n",
                "    output_dir=args.output_dir,\n",
                "    num_train_epochs=args.num_train_epochs,\n",
                "    learning_rate=args.learning_rate,\n",
                "    per_device_train_batch_size=args.batch_size,\n",
                "    gradient_checkpointing=True, # Enable for memory efficiency\n",
                "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
                "    max_grad_norm=args.max_grad_norm,\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    logging_steps=5,\n",
                "    optim=\"adamw_torch\",\n",
                "    loss_type=\"sigmoid\",\n",
                "    warmup_steps=args.warmup_steps,\n",
                "    warmup_ratio=args.warmup_ratio,\n",
                "    do_eval=False,\n",
                "    max_prompt_length=args.max_prompt_length,\n",
                "    max_length=args.max_length,\n",
                "    seed=42,\n",
                "    remove_unused_columns=False,\n",
                "    fp16=not torch.cuda.is_available(),\n",
                "    bf16=torch.cuda.is_available(),\n",
                "    beta=args.beta,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize DPOTrainer\n",
                "dpo_trainer = DPOTrainer(\n",
                "    model,\n",
                "    ref_model=None,\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=lora_config,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the model\n",
                "print(\"Starting DPO training...\")\n",
                "dpo_trainer.train()\n",
                "print(\"Training complete. Saving model...\")\n",
                "\n",
                "# Save the trained model\n",
                "final_path = os.path.join(args.output_dir, \"final_checkpoint\")\n",
                "dpo_trainer.save_model(final_path)\n",
                "print(f\"Model saved to {final_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
