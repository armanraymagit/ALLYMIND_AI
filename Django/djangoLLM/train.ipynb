{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DPO Training & Quantization Notebook (Unsloth)\n",
                "\n",
                "This notebook allows you to train or convert an LLM using Unsloth. It supports:\n",
                "- **4-bit Loading (QLoRA)** for low memory usage.\n",
                "- **GGUF Export** for use with Ollama.\n",
                "- **Vision Models** (experimental support)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "from datasets import Dataset\n",
                "from trl import DPOTrainer, DPOConfig\n",
                "from unsloth import FastLanguageModel, FastVisionModel # Import Unsloth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Config:\n",
                "    \"\"\"\n",
                "    Configuration class to replace argparse for notebook usage.\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        # Model & Data Paths\n",
                "        self.base_model_path = \"unsloth/Llama-3.2-3B-Instruct\" # Replace with your model path (e.g., 'unsloth/Llama-3.2-3B-Instruct' or 'Qwen/Qwen2.5-VL-3B-Instruct')\n",
                "        self.data_path = None # Set to 'data.json' for training\n",
                "        self.output_dir = \"./results\"\n",
                "        \n",
                "        # Training Hyperparameters\n",
                "        self.num_train_epochs = 1\n",
                "        self.learning_rate = 5e-5\n",
                "        self.batch_size = 2\n",
                "        \n",
                "        # LoRA Configuration\n",
                "        self.lora_r = 16\n",
                "        self.lora_alpha = 16\n",
                "        self.lora_dropout = 0\n",
                "        \n",
                "        # Text Generation / Processing\n",
                "        self.max_length = 1024\n",
                "        \n",
                "        # GGUF Export\n",
                "        self.save_gguf = True\n",
                "        self.quantization_method = \"q4_k_m\"\n",
                "        self.save_only = True # JSON-only (no training)\n",
                "\n",
                "args = Config()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detect if it's a vision model\n",
                "is_vision = \"vl\" in args.base_model_path.lower()\n",
                "print(f\"Loading model: {args.base_model_path} (Vision: {is_vision})\")\n",
                "\n",
                "if is_vision:\n",
                "    try:\n",
                "        model, tokenizer = FastVisionModel.from_pretrained(\n",
                "            args.base_model_path,\n",
                "            load_in_4bit = True,\n",
                "            max_seq_length = args.max_length,\n",
                "        )\n",
                "    except Exception as e:\n",
                "        print(f\"FastVisionModel failed, falling back to FastLanguageModel: {e}\")\n",
                "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "            args.base_model_path,\n",
                "            load_in_4bit = True,\n",
                "            max_seq_length = args.max_length,\n",
                "        )\n",
                "else:\n",
                "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "        model_name = args.base_model_path,\n",
                "        max_seq_length = args.max_length,\n",
                "        dtype = None,\n",
                "        load_in_4bit = True,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare training data (if training)\n",
                "if not args.save_only and args.data_path:\n",
                "    def training_data_processor(args):\n",
                "        print(f\"Loading data from {args.data_path}\")\n",
                "        return {\"prompt\": [], \"chosen\": [], \"rejected\": []} # Dummy return\n",
                "\n",
                "    data_dict = training_data_processor(args)\n",
                "    dataset = Dataset.from_dict(data_dict)\n",
                "\n",
                "    # Apply LoRA adapters\n",
                "    model = FastLanguageModel.get_peft_model(\n",
                "        model,\n",
                "        r = args.lora_r,\n",
                "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
                "        lora_alpha = args.lora_alpha,\n",
                "        lora_dropout = args.lora_dropout,\n",
                "        bias = \"none\",\n",
                "        use_gradient_checkpointing = \"unsloth\",\n",
                "    )\n",
                "\n",
                "    # Initialize DPOTrainer\n",
                "    training_args = DPOConfig(\n",
                "        output_dir=args.output_dir,\n",
                "        num_train_epochs=args.num_train_epochs,\n",
                "        learning_rate=args.learning_rate,\n",
                "        per_device_train_batch_size=args.batch_size,\n",
                "        gradient_checkpointing=True,\n",
                "        max_length=args.max_length,\n",
                "        fp16 = not torch.cuda.is_bf16_supported(),\n",
                "        bf16 = torch.cuda.is_bf16_supported(),\n",
                "    )\n",
                "\n",
                "    dpo_trainer = DPOTrainer(\n",
                "        model,\n",
                "        ref_model=None,\n",
                "        tokenizer=tokenizer,\n",
                "        args=training_args,\n",
                "        train_dataset=dataset,\n",
                "    )\n",
                "\n",
                "    print(\"Starting DPO training...\")\n",
                "    dpo_trainer.train()\n",
                "    print(\"Training complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GGUF Export\n",
                "if args.save_gguf:\n",
                "    print(f\"Exporting to GGUF ({args.quantization_method})...\")\n",
                "    try:\n",
                "        model.save_pretrained_gguf(\n",
                "            args.output_dir,\n",
                "            tokenizer,\n",
                "            quantization_method = args.quantization_method\n",
                "        )\n",
                "        print(f\"GGUF exported to {args.output_dir}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to export GGUF: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
